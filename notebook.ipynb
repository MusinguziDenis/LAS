{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from dataloader import SpeechDataset, TestSpeechDataset\n",
    "from models import LAS\n",
    "from train_test import train, validate, inference, save_model, plot_attention\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = ['<pad>', '<sos>', '<eos>', 'A',   'B',    'C',    'D', 'E',   'F',    'G',    'H',    \n",
    "         'I',   'J',    'K',    'L', 'M',   'N',    'O',    'P', 'Q',   'R',    'S',    'T', \n",
    "         'U',   'V',    'W',    'X', 'Y',   'Z',    \"'\",    ' ',]\n",
    "\n",
    "VOCAB_MAP = {VOCAB[i]:i for i in range(0, len(VOCAB))}\n",
    "PAD_TOKEN = VOCAB_MAP[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  'batch_size': 128,\n",
    "  'lr':1e-4,\n",
    "  'epochs': 50,\n",
    "  'listener_hidden_size':320,\n",
    "  'speller_embedding_dim': 256,\n",
    "  'speller_hidden_size' : 512,\n",
    "  'speller_hidden_dim': 512,\n",
    "  'projection_size': 128,\n",
    "  'max_timesteps':550,\n",
    "  'checkpoint_path': '/home/ubuntu/model/best_las.pth',\n",
    "  'epoch_checkpoint_path': '/home/ubuntu/model/epoch_las.pth'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataloaders\n",
    "train_dataset = SpeechDataset(VOCAB)\n",
    "dev_dataset   = SpeechDataset(VOCAB, partition='dev-clean')\n",
    "test_dataset  = TestSpeechDataset()\n",
    "train_loader  = DataLoader(dataset=train_dataset, num_workers=4, batch_size=config['batch_size'], shuffle=False, collate_fn=train_dataset.collate_fn)\n",
    "dev_loader    = DataLoader(dataset=dev_dataset, num_workers=4, batch_size=config['batch_size'], shuffle=False, collate_fn=dev_dataset.collate_fn)\n",
    "test_loader   = DataLoader(dataset=test_dataset, num_workers=4, batch_size=config['batch_size'], shuffle=False, collate_fn=test_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model and the optimizer\n",
    "model = LAS(listener_hidden_size=config['listener_hidden_size'],\n",
    "            speller_embedding_dim=config['speller_embedding_dim'],\n",
    "            speller_hidden_dim=config['speller_hidden_dim'],\n",
    "            speller_hidden_size=config['speller_hidden_size'],\n",
    "            projection_size=config['projection_size'],\n",
    "            max_timesteps=config['max_timesteps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer   = torch.optim.Adam(model.parameters(), lr= config['lr'])\n",
    "criterion   = torch.nn.CrossEntropyLoss(reduction='mean',ignore_index=PAD_TOKEN)\n",
    "scaler      = torch.cuda.amp.GradScaler()\n",
    "scheduler   = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode= 'min', factor =0.5, patience=3)\n",
    "tf_rate     = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    name = \"LAS model\",\n",
    "    reinit=True,\n",
    "    id= '',\n",
    "    resume= 'must',\n",
    "    project='LAS',\n",
    "    config = config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.save('LAS.txt')\n",
    "wandb.watch(model, log=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lev_dist = float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(DEVICE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in config['epochs']:\n",
    "    for param_group in optimizer.param_groups:\n",
    "        curr_lr = param_group['lr']\n",
    "        print(\"Current lr: \\t {}\".format(curr_lr))\n",
    "        \n",
    "    print(\"Start Train \\t{ Epoch}\".format(epoch))\n",
    "    startTime = time.time()\n",
    "    running_loss, running_perplexity, attention_plot= train(model, train_loader, criterion, optimizer, scaler, tf_rate)\n",
    "    print(\"Start Dev \\t{} Epoch\".format(epoch))\n",
    "    lev_dist    = validate(model, dev_loader)\n",
    "    print(\"*** Saving Checkpoint ***\")\n",
    "    save_model(model, optimizer, scheduler, ['lev_dist', lev_dist], epoch, path=config['epoch_checkpoint_path'])\n",
    "    # Print your metrics\n",
    "    print(\"\\tTrain Loss {:.04f}\\trunning_perplexity {:.04f}\\t Learning Rate {:.07f}\\t TF ratio {:.04f}\".format(running_loss, running_perplexity, curr_lr, tf_rate))\n",
    "    print(\"\\tVal Levenshtein distance {:.04f}\".format(lev_dist))\n",
    "    \n",
    "    # Plot Attention for a single item in the batch\n",
    "    plot_attention(attention_plot[0].cpu().detach().numpy())\n",
    "    \n",
    "    # Log metrics to Wandb\n",
    "    wandb.log({'Running loss': running_loss, 'Running perplexity': running_perplexity, \n",
    "               'Levenshtein distance': lev_dist, 'lr': curr_lr, 'tf_rate': tf_rate})\n",
    "    \n",
    "    scheduler.step(lev_dist)\n",
    "    \n",
    "    if lev_dist < best_lev_dist:\n",
    "        best_lev_dist = lev_dist\n",
    "        save_model(model, optimizer, scheduler, ['lev_dist', lev_dist], epoch, config['checkpoint_path'])\n",
    "        wandb.save(config['checkpoint_path'])\n",
    "        print('Saved best model')\n",
    "        \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(config['checkpoint_path'])\n",
    "model.load_state_dict(state_dict['model_state_dict'])\n",
    "model.to(DEVICE)\n",
    "\n",
    "df = inference(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
